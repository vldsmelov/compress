x-ollama-model: &ollama-model
  OLLAMA_MODEL: ${OLLAMA_MODEL:-qwen3:14b-8k}

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-all}
      NVIDIA_DRIVER_CAPABILITIES: ${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
      OLLAMA_GPU_DRIVER: ${OLLAMA_GPU_DRIVER:-cuda}
      CUDA_VERSION: ${CUDA_VERSION:-13}
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    networks:
      - app-network

  contract_extractor:
    build:
      context: .
      dockerfile: Dockerfile
      target: contract_extractor
    container_name: contract_extractor
    environment:
      <<: *ollama-model
      OLLAMA_MODEL: ${OLLAMA_MODEL:-${MODEL:-qwen3:14b-8k}}
      OLLAMA_HOST: http://ollama:11434
      MODEL: ${MODEL:-${OLLAMA_MODEL:-qwen3:14b-8k}}
      TEMPERATURE: ${TEMPERATURE:-0.1}
      MAX_TOKENS: ${MAX_TOKENS:-32000}
      NUMERIC_TOLERANCE: ${NUMERIC_TOLERANCE:-0.01}
      USE_LLM: ${USE_LLM:-true}
      API_PORT: ${API_PORT:-8085}
    command: uvicorn contract_extractor.app.main:app --host 0.0.0.0 --port 8085 --reload --log-level info
    ports:
      - "${API_PORT:-8085}:8085"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8085/healthz >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 20
    restart: unless-stopped
    depends_on:
      - ollama
    networks:
      - app-network

  ai_econom:
    build:
      context: .
      dockerfile: Dockerfile
      target: ai_econom
    container_name: ai_econom
    environment:
      <<: *ollama-model
      OLLAMA_MODEL: ${AI_ECON_OLLAMA_MODEL:-${OLLAMA_MODEL:-qwen3:14b-8k}}
      OLLAMA_HOST: ollama
      OLLAMA_PORT: 11434
    ports:
      - "10000:10000"
    volumes:
      - ./services/budget_service/data:/app/data
    restart: unless-stopped
    depends_on:
      - ollama
    networks:
      - app-network

  budget_service:
    build:
      context: .
      dockerfile: Dockerfile
      target: budget_service
    container_name: budget_service
    ports:
      - "10010:10010"
    volumes:
      - ./services/budget_service/data:/app/data
    restart: unless-stopped
    networks:
      - app-network

  ai_legal:
    build:
      context: .
      dockerfile: Dockerfile
      target: ai_legal
    container_name: ai_legal
    environment:
      <<: *ollama-model
      OLLAMA_MODEL: ${AI_LEGAL_MODEL:-${OLLAMA_MODEL:-qwen3:14b-8k}}
      OLLAMA_BASE_URL: http://ollama:11434
    ports:
      - "8092:8000"
    restart: unless-stopped
    depends_on:
      - ollama
    networks:
      - app-network

  document_slicer:
    build:
      context: .
      dockerfile: Dockerfile
      target: document_slicer
    container_name: document_slicer
    environment:
      - AI_ECONOM_SERVICE_URL=http://ai_econom:10000/analyze
      - AI_LEGAL_SERVICE_URL=http://ai_legal:8000/api/sections/full-prepared
      - CONTRACT_EXTRACTOR_URL=http://contract_extractor:8085/qa/sections?plan=default
      - SERVICE_HTTP_TIMEOUT=${SERVICE_HTTP_TIMEOUT:-120}
    ports:
      - "8090:8000"
    volumes:
      - document_data:/data
    restart: unless-stopped
    depends_on:
      - ai_econom
      - ai_legal
      - contract_extractor
    networks:
      - app-network

  admin-panel:
    build:
      context: .
      dockerfile: Dockerfile
      target: admin-panel
      args:
        VITE_SLICER_API_BASE_URL: http://localhost:8090
        VITE_AI_LEGAL_API_BASE_URL: http://localhost:8092
    container_name: admin-panel
    environment:
      - VITE_SLICER_API_BASE_URL=http://localhost:8090
      - VITE_AI_LEGAL_API_BASE_URL=http://localhost:8092
    ports:
      - "8091:80"
    restart: unless-stopped
    networks:
      - app-network

volumes:
  document_data:
  ollama_data:

networks:
  app-network:
    driver: bridge

# Микросервисный стек контрактного анализа

Стек переведён на событийную архитектуру с RabbitMQ: взаимодействие между сервисами идёт через очереди, а HTTP используется только на внешнем входе (шлюз) и в административной панели.

## Состав сервисов
- **gateway** — FastAPI-шлюз для приёма DOCX от 1С, постановки задачи в очередь и выдачи агрегированного ответа.
- **document_slicer** — подписчик очереди `doc_upload`, режет документ на 17 частей и публикует задания по частям.
- **ai_legal** — обрабатывает все части договора.
- **ai_econom** — обрабатывает только `part_16` и отправляет поле `seller` в сервис **ai_sb**.
- **ai_sb** — получает данные продавца от `ai_econom` и публикует результат для агрегации.
- **contract_extractor** — извлекает данные из каждой части договора.
- **aggregator** — собирает ответы от AI-сервисов и формирует финальный JSON, который возвращается шлюзу по reply-очереди.
- **ollama** — LLM-хост для всех AI-сервисов.
- **admin-panel** — веб-интерфейс для работы с сервисами.

## Быстрый старт
1. Убедитесь, что Docker установлен.
2. Соберите и запустите все сервисы одним compose-файлом и Dockerfile:

```bash
docker compose up -d --build
```

3. Входные точки с хоста:
   - Шлюз для загрузки договоров: http://localhost:8099
   - Панель администратора: http://localhost:8091
   - RabbitMQ management UI: http://localhost:15672 (логин/пароль по умолчанию `guest`/`guest`)
   - Ollama: http://localhost:11434

Сервисы общаются через RabbitMQ внутри сети compose по именам контейнеров. Документы складываются на сторону `document_slicer` (volume `document_data`), а результаты уходят обратно через reply-очередь, привязанную к запросу шлюза.

### Выбор модели один раз для всех сервисов

Переменная `OLLAMA_MODEL` задаёт имя модели сразу для contract_extractor, ai_econom и ai_legal. Это позволяет запустить стек на небольшой модели для проверки, а затем переключиться на «боевую» без правок в каждом сервисе:

```bash
# Поднять всё на компактной модели
OLLAMA_MODEL=qwen2:1.5b docker compose up -d --build

# После проверки просто укажите «боевую» модель и перезапустите
OLLAMA_MODEL=qwen3:14b-8k docker compose up -d
```

## Как пользоваться
- Загружайте DOCX через `/upload` шлюза (см. примеры в `curl_upload_commands.md`).
- Шлюз сам создаёт reply-очередь и дожидается финального ответа агрегатора (по умолчанию до 300 секунд; изменяется переменной
  `GATEWAY_RESPONSE_TIMEOUT`).
- Управляйте моделями и таймаутами через переменные окружения compose (например, `OLLAMA_MODEL`, `SERVICE_HTTP_TIMEOUT`, `RABBITMQ_URL`).

## Структура репозитория
```
services/
  gateway/             # FastAPI-шлюз
  document_slicer/     # нарезка документов и публикация частей
  ai_legal/            # юридический анализ частей
  ai_econom/           # экономический анализ part_16
  ai_sb/               # sb-ai обработка данных продавца
  contract_extractor/  # извлечение структурированных данных
  aggregator/          # сбор результатов и отдача reply
  admin-panel/         # фронтенд панель управления
Dockerfile             # единый multi-stage Dockerfile для всех сервисов
docker-compose.yml     # корневой compose-файл
```

Compose запускает локальный Ollama, а сервисы подключаются к нему по адресу `http://ollama:11434` внутри общей сети. Используйте переменные окружения в `docker-compose.yml`, чтобы выбрать нужную модель или адаптировать таймауты. По умолчанию сервисы ожидают модель `qwen3:14b-8k` на сервисе `ollama`.

Для моделей семейства Qwen можно управлять размером контекста через `OLLAMA_NUM_CTX` или `NUM_CTX`. Если используется `qwen3:17b` и значение явно не задано, сервисы автоматически запросят окно контекста 65 536 токенов.

## Лекция: как работает сервис (RabbitMQ-пайплайн)
1. **Приём запроса**: 1С отправляет DOCX в `POST /upload` шлюза. Шлюз создаёт задачу, открывает временную reply-очередь и публикует сообщение в очередь `doc_upload`.
2. **Нарезка**: `document_slicer` забирает задачу, режет документ на 17 частей и отправляет сообщения в очереди `ai_legal_parts`, `ai_econom_parts`, `contract_extractor_parts`. Одновременно публикует в `aggregation_tasks` ожидания по базовым сервисам (юрист/экономика/QA). Если экономический сервис найдёт продавца (`seller`), он сам инициирует цепочку `sb_ai`.
3. **Обработка частей**:
   - `ai_legal` читает все части из своей очереди и отправляет результаты в `aggregation_results`.
   - `ai_econom` обрабатывает `part_16`, публикует ответы в `aggregation_results` и, при наличии `seller`, кладёт его в `sb_queue`.
   - `ai_sb` получает `seller` из `sb_queue` и отправляет итог в `aggregation_results` **только если экономический сервис запрашивал это**.
   - `contract_extractor` извлекает структуру по каждой части и тоже пишет в `aggregation_results`.
4. **Агрегация**: `aggregator` ждёт все ожидаемые ответы для `task_id` из очереди `aggregation_results` (базово `ai_legal`, `ai_econom`, `contract_extractor`; плюс `sb_ai`, если экономический сервис отправил продавца), собирает единый JSON и публикует его в reply-очередь, указанную шлюзом.
5. **Ответ**: шлюз читает сообщение из reply-очереди и возвращает его 1С как результат запроса.

Такая схема устраняет HTTP-зависимости между сервисами и позволяет масштабировать потребителей по очередям независимо.
